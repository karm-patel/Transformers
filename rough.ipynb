{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "e63089c6",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "pd.set_option('display.max_colwidth', None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "034d2c75",
   "metadata": {},
   "outputs": [],
   "source": [
    "# df = pd.read_csv(\"dataset/hindi_english_parallel.csv\")\n",
    "# df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "31b67b96",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from datasets import load_dataset\n",
    "\n",
    "from tokenizers import Tokenizer\n",
    "from tokenizers.models import WordLevel\n",
    "from tokenizers.trainers import WordLevelTrainer\n",
    "from tokenizers.pre_tokenizers import Whitespace\n",
    "from dataloader.dataloader import NMTDataset\n",
    "%reload_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "75b56eeb",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Repo card metadata block was not found. Setting CardData to empty.\n",
      "Repo card metadata block was not found. Setting CardData to empty.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "2507"
      ]
     },
     "execution_count": 51,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_dataset = load_dataset('cfilt/iitb-english-hindi', split=\"train[:36000]\")\n",
    "train_dataset\n",
    "\n",
    "test_dataset = load_dataset('cfilt/iitb-english-hindi', split=\"test\")\n",
    "len(test_dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "b1cd55c1",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "36000"
      ]
     },
     "execution_count": 52,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(train_dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "3f2f2d07",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Repo card metadata block was not found. Setting CardData to empty.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "520"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "val_dataset = load_dataset('cfilt/iitb-english-hindi', split=\"validation\")\n",
    "len(val_dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "576f8b56",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dataloader = torch.utils.data.DataLoader(train_dataset, batch_size=len(train_dataset))\n",
    "# val_dataloader = torch.utils.data.DataLoader(train_dataset, batch_size=len(val_dataset))\n",
    "# test_dataloader = torch.utils.data.DataLoader(train_dataset, batch_size=len(test_dataset))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "9b4b1c61",
   "metadata": {},
   "outputs": [],
   "source": [
    "B = 1000\n",
    "tr = next(iter(train_dataloader))\n",
    "vl = next(iter(val_dataloader))\n",
    "tt = next(iter(test_dataloader))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "9ea5ab4d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Apply tokenizer & make vocab\n",
    "tokenizer = Tokenizer(WordLevel(unk_token=\"<unk>\"))\n",
    "tokenizer.pre_tokenizer = Whitespace()\n",
    "trainer = WordLevelTrainer(special_tokens=['<unk>', '<pad>', '<sos>', '<eos>'], min_frequency=2)\n",
    "tokenizer.train_from_iterator(tr['translation']['hi'] + vl['translation']['hi']+ tt['translation']['hi'], trainer=trainer)\n",
    "\n",
    "# save tokenizer\n",
    "tokenizer.save(\"dataset/vocab_hi.json\")\n",
    "\n",
    "# load tokenizer\n",
    "# tokenizer = Tokenizer.from_file(str(tokenizer_path))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "66089609",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Apply tokenizer & make vocab\n",
    "lang = \"en\"\n",
    "tokenizer = Tokenizer(WordLevel(unk_token=\"<unk>\"))\n",
    "tokenizer.pre_tokenizer = Whitespace()\n",
    "trainer = WordLevelTrainer(special_tokens=['<unk>', '<pad>', '<sos>', '<eos>'], min_frequency=2)\n",
    "tokenizer.train_from_iterator(tr['translation'][lang] + vl['translation'][lang]+ tt['translation'][lang], trainer=trainer)\n",
    "\n",
    "# save tokenizer\n",
    "tokenizer.save(f\"dataset/vocab_{lang}.json\")\n",
    "\n",
    "# load tokenizer\n",
    "# tokenizer = Tokenizer.from_file(str(tokenizer_path))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "7122c4f4",
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer_path = f\"dataset/vocab_hi.json\"\n",
    "tokenizer_hi = Tokenizer.from_file(tokenizer_path)\n",
    "\n",
    "tokenizer_path = f\"dataset/vocab_en.json\"\n",
    "tokenizer_en = Tokenizer.from_file(tokenizer_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "b59f0c31",
   "metadata": {},
   "outputs": [],
   "source": [
    "dataloader = torch.utils.data.DataLoader(train_dataset, batch_size=len(train_dataset))\n",
    "tokenizer_tar = tokenizer_hi\n",
    "tokenizer_src = tokenizer_en \n",
    "data = []\n",
    "\n",
    "max_src, max_tar = 0, 0\n",
    "x = next(iter(dataloader))[\"translation\"]\n",
    "for src_sent, tar_sent in zip(x[\"en\"], x[\"hi\"]):\n",
    "    src_tokens = [tokenizer_src.token_to_id(\"<sos>\")] + tokenizer_src.encode(src_sent).ids + [tokenizer_src.token_to_id(\"<eos>\")]\n",
    "    tar_tokens = [tokenizer_tar.token_to_id(\"<sos>\")] + tokenizer_tar.encode(tar_sent).ids + [tokenizer_tar.token_to_id(\"<eos>\")]\n",
    "    data.append((src_tokens, tar_tokens))\n",
    "    \n",
    "    max_src, max_tar = max(max_src, len(src_tokens)), max(max_tar, len(tar_tokens))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "id": "34b92178",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "3388"
      ]
     },
     "execution_count": 57,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer_en.get_vocab_size()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "id": "f9daf64f",
   "metadata": {},
   "outputs": [],
   "source": [
    "ds = NMTDataset(tokenizer_en, tokenizer_hi, train_dataloader)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "id": "85c7cc6f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "82"
      ]
     },
     "execution_count": 55,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(ds)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "592ba142",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(82, 100)"
      ]
     },
     "execution_count": 44,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "max_src, max_tar"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "id": "c9e49a34",
   "metadata": {},
   "outputs": [],
   "source": [
    "x = next(iter(dataloader))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "id": "323766ee",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[\"महानगर पालिका अंतर्गत दत्तात्रय नगर माध्यमिक स्कूल के विद्यार्थियों ने काल्पनिक किला 'दत्तगढ़' बनाकर अपनी कल्पनाशक्ति का परिचय दिया।\",\n",
       " 'प्रधानाध्यापक संध्या मेडपल्लीवार के प्रोत्साहित करने पर शिक्षकों व विद्यार्थियों ने मिट्टïी से किले का निर्माण किया।',\n",
       " 'मनपा शिक्षक संघ के अध्यक्ष राजेश गवरे ने स्कूल को भेंट देकर सराहना की।',\n",
       " 'किले का परीक्षण रमेश सातपुते ने किया।']"
      ]
     },
     "execution_count": 55,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x['translation']['hi'][:4]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "id": "163d23b1",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Students of the Dattatreya city Municipal corporation secondary school demonstrated their imagination power by creating the fictitious fort \"Duttgarh\".',\n",
       " 'With encouragement from Principal Sandhya Medpallivaar the teachers and students built the fort out of clay.',\n",
       " 'Rajesh Gavre, the President of the MNPA teachers association, honoured the school by presenting the award.',\n",
       " 'Ramesh Saatpute examined the fort.']"
      ]
     },
     "execution_count": 56,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x['translation']['en'][:4]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "id": "3f9abaee",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torchtext.data.utils import get_tokenizer\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from collections import Counter\n",
    "from torchtext.vocab import vocab\n",
    "from sklearn.model_selection import train_test_split\n",
    "import pandas as pd\n",
    "from torch.nn.utils.rnn import pad_sequence\n",
    "gpu_device = torch.device(\"cuda\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "id": "6499d2e0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# !pip install spacy\n",
    "# !python3 -m spacy download en_core_web_sm\n",
    "# !pip install inltk\n",
    "# import nltk\n",
    "# nltk.download('punkt')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1c8a11c5",
   "metadata": {},
   "source": [
    "### Building english vocab using Counter()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "id": "e5cb26d5",
   "metadata": {},
   "outputs": [],
   "source": [
    "en_tokenizer = get_tokenizer('spacy', language='en')   \n",
    "counter = Counter() # dict of {token: Freq}     \n",
    "for source in x['translation']['en']:\n",
    "    counter.update(en_tokenizer(source))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "id": "8db7968d",
   "metadata": {},
   "outputs": [],
   "source": [
    "voc_en = vocab(counter, specials=['<unk>', '<pad>', '<sos>', '<eos>'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "id": "59a8d733",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "162"
      ]
     },
     "execution_count": 61,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#voc_en[\"karm\"] # D:not able to handle OoV\n",
    "voc_en[\"is\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "id": "4263017d",
   "metadata": {},
   "outputs": [],
   "source": [
    "from tokenizers import Tokenizer\n",
    "tokenizer = Tokenizer.from_file(\"dataset/tokenizer_hi.json\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4cebd171",
   "metadata": {},
   "source": [
    "### Building hindi vocab using inltk and Counter()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "id": "5b499eda",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "id": "4064dc81",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "id": "dea935a5",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "id": "c4e00420",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "066f1cac",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a243a7d1",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "id": "1c8221a9",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"महानगर पालिका अंतर्गत दत्तात्रय नगर माध्यमिक स्कूल के विद्यार्थियों ने काल्पनिक किला 'दत्तगढ़' बनाकर अपनी कल्पनाशक्ति का परिचय दिया।\""
      ]
     },
     "execution_count": 62,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x['translation']['hi'][0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "id": "72c46647",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "70"
      ]
     },
     "execution_count": 71,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer.token_to_id(\"प्रोजेक्ट\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "id": "ee784ceb",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['▁महानगर',\n",
       " '▁पालिका',\n",
       " '▁अंतर्गत',\n",
       " '▁दत्ता',\n",
       " 'त्रय',\n",
       " '▁नगर',\n",
       " '▁माध्यमिक',\n",
       " '▁स्कूल',\n",
       " '▁के',\n",
       " '▁विद्यार्थियों',\n",
       " '▁ने',\n",
       " '▁काल्पनिक',\n",
       " '▁किला',\n",
       " \"▁'\",\n",
       " 'दत्त',\n",
       " 'गढ़',\n",
       " \"'\",\n",
       " '▁बनाकर',\n",
       " '▁अपनी',\n",
       " '▁कल्पना',\n",
       " 'शक्ति',\n",
       " '▁का',\n",
       " '▁परिचय',\n",
       " '▁दिया',\n",
       " '।']"
      ]
     },
     "execution_count": 63,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenize(x['translation']['hi'][0], \"hi\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "id": "745e9bfc",
   "metadata": {},
   "outputs": [],
   "source": [
    "counter = Counter() # dict of {token: Freq}     \n",
    "for source in x['translation']['hi']:\n",
    "    counter.update(tokenize(source, \"hi\"))\n",
    "voc_hi = vocab(counter, specials=['<unk>', '<pad>', '<sos>', '<eos>']) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "4364527e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "27"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "voc_hi['▁दिया']"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7336f95e",
   "metadata": {},
   "source": [
    "### Token ids"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "2376f0af",
   "metadata": {},
   "outputs": [],
   "source": [
    "token_ids = []\n",
    "for source, target in zip(x['translation']['en'], x['translation']['hi']):\n",
    "    source_tensor = torch.tensor([voc_en[\"<sos>\"]] + [voc_en[word] for word in en_tokenizer(source)] + [voc_en[\"<eos>\"]])\n",
    "    target_tensor = torch.tensor([voc_hi[\"<sos>\"]] + [voc_hi[word] for word in tokenize(target, \"hi\")] + [voc_hi[\"<eos>\"]])    \n",
    "    token_ids.append((source_tensor, target_tensor))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "43a31015",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(tensor([ 2,  4,  5,  6,  7,  8,  9, 10, 11, 12, 13, 14, 15, 16, 17, 18,  6, 19,\n",
       "          20, 21, 22, 21, 23,  3]),\n",
       "  tensor([ 2,  4,  5,  6,  7,  8,  9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20,\n",
       "          21, 22, 23, 24, 25, 26, 27, 28,  3])),\n",
       " (tensor([ 2, 24, 25, 26, 27, 28, 29,  6, 30, 31, 32, 33,  6, 20, 34,  5, 35, 23,\n",
       "           3]),\n",
       "  tensor([ 2, 29, 30, 31, 32, 33, 34, 12, 35, 36, 37, 38, 39, 13, 14, 40, 41, 42,\n",
       "          43, 44, 45, 25, 46, 47, 28,  3]))]"
      ]
     },
     "execution_count": 47,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "token_ids[:2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f488117a",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ca80506f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torchtext.data.utils import get_tokenizer\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from collections import Counter\n",
    "from torchtext.vocab import vocab\n",
    "from sklearn.model_selection import train_test_split\n",
    "import pandas as pd\n",
    "from torch.nn.utils.rnn import pad_sequence\n",
    "gpu_device = torch.device(\"cuda\")\n",
    "\n",
    "# todos 1) Make generalize tokenizer 2) make preprocessing different so we can use while custom dataset\n",
    "class DateDataset(Dataset):\n",
    "    def __init__(self, DATASET_PATH = \"dataset/Assignment2aDataset.txt\", split=\"train\"):\n",
    "        df = pd.read_csv(DATASET_PATH, names = [\"source\", \"target\"])\n",
    "        df[\"source\"] = df[\"source\"].apply(lambda x: x.strip()[1:-1].replace(\"/\", \"-\"))\n",
    "        df[\"target\"] = df[\"target\"].apply(lambda x: x.strip()[1:-1])\n",
    "        df_train, df_test = train_test_split(df, random_state=42, test_size=0.1)\n",
    "        \n",
    "        # tokenize\n",
    "        en_tokenizer = get_tokenizer('spacy', language='en')   \n",
    "        counter = Counter() # dict of {token: Freq}     \n",
    "        for source in df[\"source\"]:\n",
    "            counter.update(en_tokenizer(source))\n",
    "\n",
    "        for source in df[\"target\"]:\n",
    "            counter.update(en_tokenizer(source))\n",
    "        \n",
    "        voc = vocab(counter, specials=['<unk>', '<pad>', '<bos>', '<eos>'])    \n",
    "        \n",
    "        # create data\n",
    "        if split == \"train\":\n",
    "            self.data_df = df_train\n",
    "        else:\n",
    "            self.data_df = df_test\n",
    "            \n",
    "        data = []\n",
    "        for (source, target) in zip(self.data_df[\"source\"], self.data_df[\"target\"]):\n",
    "            s_tensor_ = torch.tensor([voc[token] for token in en_tokenizer(source)])\n",
    "            t_tensor_ = torch.tensor([voc[token] for token in en_tokenizer(target)])\n",
    "            data.append((s_tensor_, t_tensor_))\n",
    "        \n",
    "        self.voc = voc\n",
    "        self.data = data\n",
    "    \n",
    "    def __len__(self):\n",
    "        return len(self.data)\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        return self.data[idx]\\\n",
    "        \n",
    "\n",
    "def generate_batch(data_batch, special_tokens):\n",
    "    BOS_IDX = special_tokens[\"BOS_IDX\"]\n",
    "    PAD_IDX = special_tokens[\"PAD_IDX\"]\n",
    "    EOS_IDX = special_tokens[\"EOS_IDX\"]\n",
    "\n",
    "    s_batch, t_batch = [], []\n",
    "    for (s_item, t_item) in data_batch:\n",
    "        s_batch.append(torch.cat([torch.tensor([BOS_IDX]), s_item, torch.tensor([EOS_IDX])], dim=0))\n",
    "        t_batch.append(torch.cat([torch.tensor([BOS_IDX]), t_item, torch.tensor([EOS_IDX])], dim=0))\n",
    "        \n",
    "    s_batch = pad_sequence(s_batch, padding_value=PAD_IDX)\n",
    "    return s_batch.T.to(gpu_device), torch.stack(t_batch).to(gpu_device)\n",
    "\n",
    "def get_dataloader(split=\"train\", batch_size=4000):\n",
    "    dataset = DateDataset(split=split)\n",
    "    special_tokens = {}\n",
    "    special_tokens[\"BOS_IDX\"] = dataset.voc[\"<bos>\"]\n",
    "    special_tokens[\"EOS_IDX\"] = dataset.voc[\"<eos>\"]\n",
    "    special_tokens[\"PAD_IDX\"] = dataset.voc[\"<pad>\"]\n",
    "\n",
    "    dataloader = DataLoader(dataset, batch_size=batch_size, collate_fn=lambda batch, \n",
    "                            sp_tokens = special_tokens : generate_batch(batch, sp_tokens))\n",
    "\n",
    "    return dataset, dataloader\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8c403995",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1fa48579",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6e7c7792",
   "metadata": {},
   "outputs": [],
   "source": [
    "# tokenizer = AutoTokenizer.from_pretrained('bert-base-cased')\n",
    "# dataset = dataset.map(lambda e: tokenizer(e['sentence1']), batched=True)\n",
    "# dataset.set_format(type='torch', columns=['input_ids', 'token_type_ids', 'attention_mask', 'labels'])\n",
    "# dataloader = torch.utils.data.DataLoader(dataset, batch_size=32)\n",
    "# next(iter(dataloader))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "15d615d9",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "854ffbe6",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:torch310]",
   "language": "python",
   "name": "conda-env-torch310-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
